# llm_hallucinations

Проект для тестирования галюцинаций LLM-моделей

LLM-галюцинации - это случаи, когда модель генерирует фактически неверный или выдуманный контент, который не соответствует фактам, исходным данным или контексту.

Виды галюцинаций:
1) Фактологические (Ленин - гриб)
2) Контекстные (неправильно определен главный герой сказки)
3) Логические (3 яйца варятся за 3 минуты, значит 1 яйцо за 1 минуту)
4) Несоответствие инструкции (отвечает больше, чем в одно слово, чем прямо нарушает поставленную тобой инструкцию)

Причины галюцинаций:
1) Некорректные данные при обучении
2) Общественные предрассудки
3) Дубликаты при обучении
4) Ограниченность знаний
5) Желание модели ответить хоть что-то, так как ответ "не знаю" недопустим
6) Особенности механизма self-attention

Мы составили 3 датасета для проверки каждого вида галюцинаций: фактологический, контекстный и логический.
Каждый датасет содержит системный промпт, а также два столбца: prompt (вопрос), answer (лучший/правильный ответ)
Модель судья оценивает:  
1) правильно ли тестируемая модель ответила на вопрос 
2) выполнены ли инструкции (4-й вид галюцинации)
